#20.02.2026
volumes:
  ny_taxi_postgres_data:
    driver: local
  kestra_postgres_data:
    driver: local
  kestra_data:
    driver: local

networks:
  kestra-spark-network:
    name: kestra-spark-network
    driver: bridge

services:
  # spark-master:
  #   image: apache/spark:4.0.1-java21-r
  #   container_name: spark-master
  #   command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
  #   ports:
  #     - "38080:8080"   # Spark Master UI
  #     - "37077:7077"   # Spark Master RPC
  #   volumes:
  #     - /workspaces/kestra/service-account.json:/opt/spark/conf/gcp-key.json:ro
  #     - ./gcs_spark.py:/opt/spark/work-dir/gcs_spark.py
  #     - ./gcs-connector-4.0.1-shaded.jar:/opt/spark/jars/gcs-connector-4.0.1-shaded.jar:ro
  #   environment:
  #     - GOOGLE_APPLICATION_CREDENTIALS=/opt/spark/conf/gcp-key.json   # harmless, but Hadoop confs in app are what matter
  #   networks:
  #     - kestra-spark-network
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8080"]
  #     interval: 30s
  #     timeout: 5s
  #     retries: 5

  # spark-worker:
  #   image: apache/spark:4.0.1-java21-r
  #   container_name: spark-worker
  #   depends_on:
  #     - spark-master
  #   command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
  #   environment:
  #     - SPARK_WORKER_CORES=1
  #     - SPARK_WORKER_MEMORY=1G
  #     - GOOGLE_APPLICATION_CREDENTIALS=/opt/spark/conf/gcp-key.json
  #   ports:
  #     - "38081:8081"   # Worker UI
  #   volumes:
  #     - /workspaces/kestra/service-account.json:/opt/spark/conf/gcp-key.json:ro
  #     - ./gcs-connector-4.0.1-shaded.jar:/opt/spark/jars/gcs-connector-4.0.1-shaded.jar:ro
  #   networks:
  #     - kestra-spark-network
  #   restart: unless-stopped
  spark-master:
    image: apache/spark:4.0.1-java21-r
    container_name: spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    ports:
      - "38080:8080"
      - "37077:7077"
    environment:
      # These fix the "getSubject" error and Java 21 modularity issues
      - SPARK_MASTER_OPTS=--add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/javax.security.auth=ALL-UNNAMED -Djdk.security.allowNonCausingSubstitutions=true
      - SPARK_DRIVER_JAVA_OPTS=-Djdk.security.allowNonCausingSubstitutions=true
    volumes:
      # Use relative paths for better portability
      - ./service-account.json:/opt/spark/conf/gcp-key.json:ro
      - ./pyspark/gcs_spark.py:/opt/spark/work-dir/gcs_spark.py
      #- ./gcs-connector-4.0.1-shaded.jar:/opt/spark/jars/gcs-connector-4.0.1-shaded.jar:ro
    networks:
      - kestra-spark-network
    restart: unless-stopped

  spark-worker:
    image: apache/spark:4.0.1-java21-r
    container_name: spark-worker
    depends_on:
      - spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      # These flags ensure the worker can handle the Java 21 security requirements
      - SPARK_WORKER_OPTS=--add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/javax.security.auth=ALL-UNNAMED -Djdk.security.allowNonCausingSubstitutions=true
      - SPARK_EXECUTOR_JAVA_OPTS=-Djdk.security.allowNonCausingSubstitutions=true
    volumes:
      - ./service-account.json:/opt/spark/conf/gcp-key.json:ro
      #- ./gcs-connector-4.0.1-shaded.jar:/opt/spark/jars/gcs-connector-4.0.1-shaded.jar:ro
    networks:
      - kestra-spark-network
    restart: unless-stopped

  # --- KESTRA INFRASTRUCTURE ---
  kestra_postgres:
    image: postgres:18
    volumes:
      - kestra_postgres_data:/var/lib/postgresql
    environment:
      POSTGRES_DB: kestra
      POSTGRES_USER: kestra
      POSTGRES_PASSWORD: k3str4
    networks:
      - kestra-spark-network

  kestra:
    image: kestra/kestra:v1.1
    user: "root"
    command: server standalone
    volumes:
      - kestra_data:/app/storage
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/kestra-wd:/tmp/kestra-wd
      - ~/.gcp/workflow-orchestration-credentials.json:/.gcp/credentials.json
    env_file: .env_encoded
    environment:
      KESTRA_CONFIGURATION: |
        datasources:
          postgres:
            url: jdbc:postgresql://kestra_postgres:5432/kestra
            driverClassName: org.postgresql.Driver
            username: kestra
            password: k3str4
        kestra:
          storage:
            type: local
            local:
              basePath: "/app/storage"
          repository:
            type: postgres
          queue:
            type: postgres
        tasks:
          scripts:
            docker:
              volumes:
                enabled: true
    ports:
      - "8080:8080"
      - "8081:8081"
    networks:
      - kestra-spark-network
    depends_on:
      kestra_postgres:
        condition: service_started

  # (Your other services like pgdatabase and pgadmin should also join 'kestra-spark-network')



